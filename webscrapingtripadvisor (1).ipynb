{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df45fb-e88b-4bc2-bb86-5488ca05563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "def setup_request():\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "\n",
    "def get_restaurant_info(soup):\n",
    "    restaurant_info = {\n",
    "        'name': '',\n",
    "        'price_level': '',\n",
    "        'cuisine_type': '',\n",
    "        'total_rating': '',\n",
    "        'total_reviews': '',\n",
    "        'food_rating': '',\n",
    "        'service_rating': '',\n",
    "        'value_rating': '',\n",
    "        'atmosphere_rating': '',\n",
    "        'ranking': '',\n",
    "        'city': '',\n",
    "        'address': '',\n",
    "        'phone_no': ''\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        restaurant_info['name'] = soup.find('h1').text.strip()\n",
    "    except:\n",
    "        restaurant_info['name'] = 'N/A'\n",
    "\n",
    "    try:\n",
    "        general_infos = soup.find('span', class_='rRtyp').text.strip()\n",
    "        info_parts = general_infos.split(', ')\n",
    "        restaurant_info['price_level'] = info_parts[0]\n",
    "        restaurant_info['cuisine_type'] = ', '.join(info_parts[1:])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    detail_cards = soup.find_all('div', attrs={'data-automation': 'OVERVIEW_TAB_ELEMENT'})\n",
    "    if detail_cards:\n",
    "        rating_info = detail_cards[0]\n",
    "        try:\n",
    "            restaurant_info['total_rating'] = rating_info.find('span', class_='biGQs').text.strip()\n",
    "            reviews_text = rating_info.find('div', class_='KxBGd').text.strip()\n",
    "            restaurant_info['total_reviews'] = reviews_text.replace(' reviews', '').replace(',', '')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            rating_container = rating_info.find('div', class_='khxWm')\n",
    "            if rating_container:\n",
    "                rating_category = rating_container.find_all('div', class_='YwaWb')\n",
    "                if len(rating_category) >= 4:\n",
    "                    restaurant_info['food_rating'] = rating_category[0].find('svg', class_='UctUV').find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "                    restaurant_info['service_rating'] = rating_category[1].find('svg', class_='UctUV').find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "                    restaurant_info['value_rating'] = rating_category[2].find('svg', class_='UctUV').find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "                    restaurant_info['atmosphere_rating'] = rating_category[3].find('svg', class_='UctUV').find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting detailed ratings: {str(e)}\")\n",
    "\n",
    "        try:\n",
    "            ranking_tag = rating_info.find_all('a', class_='BMQDV')\n",
    "            if len(ranking_tag) > 1:\n",
    "                ranking_text = ranking_tag[1].find('span').text.strip()\n",
    "                restaurant_info['ranking'] = ranking_text.split()[0].replace('#', '')\n",
    "                in_index = ranking_text.split().index('in')\n",
    "                restaurant_info['city'] = ' '.join(ranking_text.split()[in_index + 1:])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if len(detail_cards) > 2:\n",
    "        location_info = detail_cards[2]\n",
    "        try:\n",
    "            restaurant_info['address'] = location_info.find('span', class_='biGQs').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            phone_link = location_info.find('a', attrs={'aria-label': 'Call'})\n",
    "            if phone_link:\n",
    "                restaurant_info['phone_no'] = phone_link.get('href').replace('tel:', '')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting phone number: {str(e)}\")\n",
    "\n",
    "    return restaurant_info\n",
    "\n",
    "\n",
    "def scrape_reviews(soup):\n",
    "    reviews = []\n",
    "    review_cards = soup.find_all('div', attrs={'data-automation': 'reviewCard'})\n",
    "\n",
    "    for review in review_cards:\n",
    "        review_data = {\n",
    "            'rating': '',\n",
    "            'title': '',\n",
    "            'text': '',\n",
    "            'date': ''\n",
    "        }\n",
    "\n",
    "        rating_element = review.find('svg', class_='UctUV')\n",
    "        if rating_element:\n",
    "            review_data['rating'] = rating_element.find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "\n",
    "        title_element = review.find('div', attrs={'data-test-target': 'review-title'})\n",
    "        if title_element:\n",
    "            review_data['title'] = title_element.text.strip()\n",
    "\n",
    "        text_element = review.find('div', attrs={'data-test-target': 'review-body'})\n",
    "        if text_element:\n",
    "            review_data['text'] = text_element.text.strip()\n",
    "\n",
    "        date_element = review.find('div', class_='neAPm')\n",
    "        if date_element:\n",
    "            child_divs = date_element.find_all('div')\n",
    "            if child_divs:\n",
    "                review_data['date'] = child_divs[0].text.strip().replace('Written ', '')\n",
    "\n",
    "        reviews.append(review_data)\n",
    "        time.sleep(1)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def generate_review_urls(base_url, total_reviews, reviews_per_page=10):\n",
    "    pages = range(0, total_reviews, reviews_per_page)\n",
    "    urls = []\n",
    "    for page in pages:\n",
    "        if page == 0:\n",
    "            urls.append(base_url)\n",
    "        else:\n",
    "            paginated_url = base_url.replace(\"Reviews-\", f\"Reviews-or{page}-\")\n",
    "            urls.append(paginated_url)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def save_to_csv(restaurant_info, reviews, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        header = ['RESTAURANT_NAME', 'PRICE_LEVEL', 'CUISINE_TYPE', 'TOTAL_RATING',\n",
    "                  'TOTAL_REVIEWS', 'FOOD_RATING', 'SERVICE_RATING', 'VALUE_RATING',\n",
    "                  'ATMOSPHERE_RATING', 'RANKING', 'CITY', 'ADDRESS', 'PHONE_NO',\n",
    "                  'RATING', 'REVIEW_TITLE', 'REVIEW_TEXT', 'REVIEW_DATE']\n",
    "        writer.writerow(header)\n",
    "\n",
    "        for review in reviews:\n",
    "            row = [\n",
    "                restaurant_info['name'],\n",
    "                restaurant_info['price_level'],\n",
    "                restaurant_info['cuisine_type'],\n",
    "                restaurant_info['total_rating'],\n",
    "                restaurant_info['total_reviews'],\n",
    "                restaurant_info['food_rating'],\n",
    "                restaurant_info['service_rating'],\n",
    "                restaurant_info['value_rating'],\n",
    "                restaurant_info['atmosphere_rating'],\n",
    "                restaurant_info['ranking'],\n",
    "                restaurant_info['city'],\n",
    "                restaurant_info['address'],\n",
    "                restaurant_info['phone_no'],\n",
    "                review['rating'],\n",
    "                review['title'],\n",
    "                review['text'],\n",
    "                review['date']\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_url = 'https://www.tripadvisor.com.my/Restaurant_Review-g298313-d15006574-Reviews-Mean_Mince-Petaling_Jaya_Petaling_District_Selangor.html'\n",
    "    headers = setup_request()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        time.sleep(5)\n",
    "\n",
    "        restaurant_info = get_restaurant_info(soup)\n",
    "        total_reviews = int(restaurant_info['total_reviews'])\n",
    "\n",
    "        print(f\"Total reviews found: {total_reviews}\")\n",
    "        urls = generate_review_urls(base_url, total_reviews)\n",
    "\n",
    "        all_reviews = []\n",
    "\n",
    "        for url in urls:\n",
    "            print(f\"Scraping: {url}\")\n",
    "            res = requests.get(url, headers=headers)\n",
    "            page_soup = BeautifulSoup(res.content, 'html.parser')\n",
    "            reviews = scrape_reviews(page_soup)\n",
    "            all_reviews.extend(reviews)\n",
    "            time.sleep(3)\n",
    "\n",
    "        save_to_csv(restaurant_info, all_reviews, 'mince_meat_reviews_all.csv')\n",
    "        print(\"✅ All reviews scraped and saved!\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error during requests to {base_url} : {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5ea7b-57f3-417b-9546-92ff0073e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "def setup_request():\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    }\n",
    "\n",
    "    return headers\n",
    "\n",
    "\n",
    "def get_restaurant_info(soup):\n",
    "    restaurant_info = {\n",
    "        'name': '',\n",
    "        'price_level': '',\n",
    "        'cuisine_type': '',\n",
    "        'total_rating': '',\n",
    "        'total_reviews': '',\n",
    "        'food_rating': '',\n",
    "        'service_rating': '',\n",
    "        'value_rating': '',\n",
    "        'atmosphere_rating': '',\n",
    "        'ranking': '',\n",
    "        'city': '',\n",
    "        'address': '',\n",
    "        'phone_no': ''\n",
    "    }\n",
    "\n",
    "    restaurant_info['name'] = soup.find('h1').text.strip()\n",
    "\n",
    "    # General info processing\n",
    "    general_infos = soup.find('span', class_='KxBGd').text.strip()\n",
    "    info_parts = general_infos.split(', ')\n",
    "    restaurant_info['price_level'] = info_parts[0]\n",
    "    restaurant_info['cuisine_type'] = ', '.join(info_parts[1:])\n",
    "\n",
    "    # Rating and review info\n",
    "    detail_cards = soup.find_all(\n",
    "        'div', attrs={'data-automation': 'OVERVIEW_TAB_ELEMENT'})\n",
    "    if detail_cards:\n",
    "        rating_info = detail_cards[0]\n",
    "        restaurant_info['total_rating'] = rating_info.find(\n",
    "            'span', class_='uuBRH').text.strip()\n",
    "        reviews_text = rating_info.find('div', class_='KxBGd').text.strip()\n",
    "        restaurant_info['total_reviews'] = reviews_text.replace(' reviews', '')\n",
    "\n",
    "        # Detailed ratings\n",
    "        try:\n",
    "            rating_container = rating_info.find('div', class_='khxWm')\n",
    "            if rating_container:\n",
    "                rating_category = rating_container.find_all(\n",
    "                    'div', class_='YwaWb')\n",
    "                if len(rating_category) >= 4:\n",
    "                    restaurant_info['food_rating'] = rating_category[0].find(\n",
    "                        'svg', class_='UctUV').find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "                    restaurant_info['service_rating'] = rating_category[1].find(\n",
    "                        'svg', class_='UctUV').find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "                    restaurant_info['value_rating'] = rating_category[2].find(\n",
    "                        'svg', class_='UctUV').find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "                    restaurant_info['atmosphere_rating'] = rating_category[3].find(\n",
    "                        'svg', class_='UctUV').find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting detailed ratings: {str(e)}\")\n",
    "\n",
    "        # Ranking and city info\n",
    "        ranking_tag = rating_info.find_all('a', class_='ffHql')\n",
    "        if len(ranking_tag) > 1:\n",
    "            ranking_text = ranking_tag[1].find('span').text.strip()\n",
    "            restaurant_info['ranking'] = ranking_text.split()[\n",
    "                0].replace('#', '')\n",
    "            in_index = ranking_text.split().index('in')\n",
    "            restaurant_info['city'] = ' '.join(\n",
    "                ranking_text.split()[in_index + 1:])\n",
    "\n",
    "    # Address and phone info\n",
    "    if len(detail_cards) > 2:\n",
    "        location_info = detail_cards[2]\n",
    "        restaurant_info['address'] = location_info.find(\n",
    "            'span', class_='biGQs').text.strip()\n",
    "\n",
    "        # Phone number\n",
    "        try:\n",
    "            phone_link = location_info.find('a', attrs={'aria-label': 'Call'})\n",
    "            if phone_link:\n",
    "                restaurant_info['phone_no'] = phone_link.get(\n",
    "                    'href').replace('tel:', '')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting phone number: {str(e)}\")\n",
    "\n",
    "    return restaurant_info\n",
    "\n",
    "\n",
    "def scrape_reviews(soup):\n",
    "    reviews = []\n",
    "    review_cards = soup.find_all(\n",
    "        'div', attrs={'data-automation': 'reviewCard'})\n",
    "\n",
    "    for review in review_cards:\n",
    "        review_data = {\n",
    "            'rating': '',\n",
    "            'title': '',\n",
    "            'text': '',\n",
    "            'date': ''\n",
    "        }\n",
    "\n",
    "        rating_element = review.find('svg', class_='UctUV')\n",
    "        if rating_element:\n",
    "            review_data['rating'] = rating_element.find(\n",
    "                'title').text.strip().replace(' of 5 bubbles', '')\n",
    "\n",
    "        title_element = review.find(\n",
    "            'div', attrs={'data-test-target': 'review-title'})\n",
    "        if title_element:\n",
    "            review_data['title'] = title_element.text.strip()\n",
    "\n",
    "        text_element = review.find(\n",
    "            'div', attrs={'data-test-target': 'review-body'})\n",
    "        if text_element:\n",
    "            review_data['text'] = text_element.text.strip()\n",
    "\n",
    "        date_element = review.find('div', class_='neAPm')\n",
    "        if date_element:\n",
    "            child_divs = date_element.find_all('div')\n",
    "            if child_divs:\n",
    "                review_data['date'] = child_divs[0].text.strip().replace(\n",
    "                    'Written ', '')\n",
    "\n",
    "        reviews.append(review_data)\n",
    "        time.sleep(3)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def save_to_csv(restaurant_info, reviews, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write header\n",
    "        header = ['RESTAURANT_NAME', 'PRICE_LEVEL', 'CUISINE_TYPE', 'TOTAL_RATING',\n",
    "                  'TOTAL_REVIEWS', 'FOOD_RATING', 'SERVICE_RATING', 'VALUE_RATING',\n",
    "                  'ATMOSPHERE_RATING', 'RANKING', 'CITY', 'ADDRESS', 'PHONE_NO',\n",
    "                  'RATING', 'REVIEW_TITLE', 'REVIEW_TEXT', 'REVIEW_DATE']\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Write reviews with restaurant info\n",
    "        for review in reviews:\n",
    "            row = [\n",
    "                restaurant_info['name'],\n",
    "                restaurant_info['price_level'],\n",
    "                restaurant_info['cuisine_type'],\n",
    "                restaurant_info['total_rating'],\n",
    "                restaurant_info['total_reviews'],\n",
    "                restaurant_info['food_rating'],\n",
    "                restaurant_info['service_rating'],\n",
    "                restaurant_info['value_rating'],\n",
    "                restaurant_info['atmosphere_rating'],\n",
    "                restaurant_info['ranking'],\n",
    "                restaurant_info['city'],\n",
    "                restaurant_info['address'],\n",
    "                restaurant_info['phone_no'],\n",
    "                review['rating'],\n",
    "                review['title'],\n",
    "                review['text'],\n",
    "                review['date']\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.tripadvisor.com.my/Restaurant_Review-g298317-d16090258-Reviews-Spade_s_Burger-Subang_Jaya_Petaling_District_Selangor.html'\n",
    "    headers = setup_request()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        time.sleep(10)\n",
    "\n",
    "        restaurant_info = get_restaurant_info(soup)\n",
    "        reviews = scrape_reviews(soup)\n",
    "\n",
    "        save_to_csv(restaurant_info, reviews,\n",
    "                    'spades_burger_reviews.csv')\n",
    "        print(\"All information saved successfully\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error during requests to {url} : {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ae1ce-ee32-4974-8bda-fe523200ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "def setup_request():\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "\n",
    "def get_restaurant_name(soup):\n",
    "    try:\n",
    "        return soup.find('h1').text.strip()\n",
    "    except:\n",
    "        return 'N/A'\n",
    "\n",
    "\n",
    "def scrape_reviews(soup):\n",
    "    reviews = []\n",
    "    review_cards = soup.find_all('div', attrs={'data-automation': 'reviewCard'})\n",
    "\n",
    "    for review in review_cards:\n",
    "        review_data = {\n",
    "            'rating': '',\n",
    "            'title': '',\n",
    "            'text': '',\n",
    "            'date': ''\n",
    "        }\n",
    "\n",
    "        rating_element = review.find('svg', class_='UctUV')\n",
    "        if rating_element:\n",
    "            review_data['rating'] = rating_element.find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "\n",
    "        title_element = review.find('div', attrs={'data-test-target': 'review-title'})\n",
    "        if title_element:\n",
    "            review_data['title'] = title_element.text.strip()\n",
    "\n",
    "        text_element = review.find('div', attrs={'data-test-target': 'review-body'})\n",
    "        if text_element:\n",
    "            review_data['text'] = text_element.text.strip()\n",
    "\n",
    "        date_element = review.find('div', class_='neAPm')\n",
    "        if date_element:\n",
    "            child_divs = date_element.find_all('div')\n",
    "            if child_divs:\n",
    "                review_data['date'] = child_divs[0].text.strip().replace('Written ', '')\n",
    "\n",
    "        reviews.append(review_data)\n",
    "        time.sleep(1)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def save_to_csv(restaurant_name, reviews, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['RESTAURANT_NAME', 'RATING', 'REVIEW_TITLE', 'REVIEW_TEXT', 'REVIEW_DATE'])\n",
    "\n",
    "        for review in reviews:\n",
    "            writer.writerow([\n",
    "                restaurant_name,\n",
    "                review['rating'],\n",
    "                review['title'],\n",
    "                review['text'],\n",
    "                review['date']\n",
    "            ])\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.tripadvisor.com.my/Restaurant_Review-g298570-d27935260-Reviews-Woodfire_Kl_ttdi-Kuala_Lumpur_Wilayah_Persekutuan.html'\n",
    "    headers = setup_request()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        time.sleep(5)\n",
    "\n",
    "        restaurant_name = get_restaurant_name(soup)\n",
    "        reviews = scrape_reviews(soup)\n",
    "\n",
    "        save_to_csv(restaurant_name, reviews, 'woodfire_kl_reviews.csv')\n",
    "        print(f\"✅ Reviews for '{restaurant_name}' saved successfully!\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error during requests to {url} : {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612aa9c4-57c3-43f9-9f4e-43f27f5bc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "def setup_request():\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "\n",
    "def get_restaurant_name(soup):\n",
    "    try:\n",
    "        return soup.find('h1').text.strip()\n",
    "    except:\n",
    "        return 'N/A'\n",
    "\n",
    "\n",
    "def scrape_reviews(soup):\n",
    "    reviews = []\n",
    "    review_cards = soup.find_all('div', attrs={'data-automation': 'reviewCard'})\n",
    "\n",
    "    for review in review_cards:\n",
    "        review_data = {\n",
    "            'rating': '',\n",
    "            'title': '',\n",
    "            'text': '',\n",
    "            'date': ''\n",
    "        }\n",
    "\n",
    "        rating_element = review.find('svg', class_='UctUV')\n",
    "        if rating_element:\n",
    "            review_data['rating'] = rating_element.find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "\n",
    "        title_element = review.find('div', attrs={'data-test-target': 'review-title'})\n",
    "        if title_element:\n",
    "            review_data['title'] = title_element.text.strip()\n",
    "\n",
    "        text_element = review.find('div', attrs={'data-test-target': 'review-body'})\n",
    "        if text_element:\n",
    "            review_data['text'] = text_element.text.strip()\n",
    "\n",
    "        date_element = review.find('div', class_='neAPm')\n",
    "        if date_element:\n",
    "            child_divs = date_element.find_all('div')\n",
    "            if child_divs:\n",
    "                review_data['date'] = child_divs[0].text.strip().replace('Written ', '')\n",
    "\n",
    "        reviews.append(review_data)\n",
    "        time.sleep(1)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def get_next_page(soup):\n",
    "    try:\n",
    "        # Find the \"Next\" page link by looking for the \"next\" button or pagination link\n",
    "        next_button = soup.find('a', class_='ui_button nav next primary')\n",
    "        if next_button and 'href' in next_button.attrs:\n",
    "            return next_button['href']\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_to_csv(restaurant_name, reviews, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['RESTAURANT_NAME', 'RATING', 'REVIEW_TITLE', 'REVIEW_TEXT', 'REVIEW_DATE'])\n",
    "\n",
    "        for review in reviews:\n",
    "            writer.writerow([\n",
    "                restaurant_name,\n",
    "                review['rating'],\n",
    "                review['title'],\n",
    "                review['text'],\n",
    "                review['date']\n",
    "            ])\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.tripadvisor.com.my/Restaurant_Review-g298570-d25153683-Reviews-Burger_And_Lobster_Klcc-Kuala_Lumpur_Wilayah_Persekutuan.html'\n",
    "    headers = setup_request()\n",
    "\n",
    "    all_reviews = []\n",
    "    page_number = 1\n",
    "\n",
    "    try:\n",
    "        while url:\n",
    "            print(f\"Scraping page {page_number}...\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            restaurant_name = get_restaurant_name(soup)\n",
    "            reviews = scrape_reviews(soup)\n",
    "\n",
    "            all_reviews.extend(reviews)  # Append new reviews to the list\n",
    "\n",
    "            # Check if there's a next page\n",
    "            url = get_next_page(soup)\n",
    "            if url:\n",
    "                # Ensure we get the full URL if it's a relative link\n",
    "                if not url.startswith('http'):\n",
    "                    base_url = 'https://www.tripadvisor.com.my'\n",
    "                    url = base_url + url\n",
    "                page_number += 1\n",
    "                time.sleep(2)  # Respectful delay between requests\n",
    "            else:\n",
    "                print(\"No more pages found.\")\n",
    "                break\n",
    "\n",
    "        # After scraping all pages, save the reviews to a CSV file\n",
    "        save_to_csv(restaurant_name, all_reviews, 'burger_and_lobster_kl_all_reviews.csv')\n",
    "        print(f\"✅ All reviews for '{restaurant_name}' saved successfully!\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error during requests to {url} : {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a39c29c-9592-46ff-82f5-97a126aab2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reviews for 'myBurgerLab' saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def setup_request():\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "def get_restaurant_name(soup):\n",
    "    try:\n",
    "        return soup.find('h1').text.strip()\n",
    "    except:\n",
    "        return 'N/A'\n",
    "\n",
    "def scrape_reviews(soup):\n",
    "    reviews = []\n",
    "    review_cards = soup.find_all('div', attrs={'data-automation': 'reviewCard'})\n",
    "\n",
    "    for review in review_cards:\n",
    "        review_data = {\n",
    "            'rating': '',\n",
    "            'title': '',\n",
    "            'text': '',\n",
    "            'date': ''\n",
    "        }\n",
    "\n",
    "        rating_element = review.find('svg', class_='UctUV')\n",
    "        if rating_element:\n",
    "            review_data['rating'] = rating_element.find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "\n",
    "        title_element = review.find('div', attrs={'data-test-target': 'review-title'})\n",
    "        if title_element:\n",
    "            review_data['title'] = title_element.text.strip()\n",
    "\n",
    "        text_element = review.find('div', attrs={'data-test-target': 'review-body'})\n",
    "        if text_element:\n",
    "            review_data['text'] = text_element.text.strip()\n",
    "\n",
    "        date_element = review.find('div', class_='neAPm')\n",
    "        if date_element:\n",
    "            child_divs = date_element.find_all('div')\n",
    "            if child_divs:\n",
    "                review_data['date'] = child_divs[0].text.strip().replace('Written ', '')\n",
    "\n",
    "        reviews.append(review_data)\n",
    "        time.sleep(1)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def save_to_csv(restaurant_name, reviews, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['RESTAURANT_NAME', 'RATING', 'REVIEW_TITLE', 'REVIEW_TEXT', 'REVIEW_DATE'])\n",
    "\n",
    "        for review in reviews:\n",
    "            writer.writerow([\n",
    "                restaurant_name,\n",
    "                review['rating'],\n",
    "                review['title'],\n",
    "                review['text'],\n",
    "                review['date']\n",
    "            ])\n",
    "\n",
    "def scrape_all_reviews(base_url):\n",
    "    page_number = 0\n",
    "    all_reviews = []\n",
    "    while True:\n",
    "        url = f\"{base_url}&start={page_number}\"\n",
    "        headers = setup_request()\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Scrape reviews from this page\n",
    "            reviews = scrape_reviews(soup)\n",
    "            if not reviews:\n",
    "                break  # Stop if no reviews are found (i.e., we've reached the last page)\n",
    "\n",
    "            all_reviews.extend(reviews)\n",
    "\n",
    "            # Check if there's a next page\n",
    "            next_page = soup.find('a', class_='unMKR')\n",
    "            if next_page:\n",
    "                page_number += 10  # Increment by 10 (the standard increment for page navigation on TripAdvisor)\n",
    "            else:\n",
    "                break  # No more pages, stop the loop\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error during requests to {url} : {str(e)}\")\n",
    "            break\n",
    "\n",
    "    return all_reviews\n",
    "\n",
    "def main():\n",
    "    base_url = 'https://www.tripadvisor.com.my/Restaurant_Review-g298313-d7281339-Reviews-MyBurgerLab-Petaling_Jaya_Petaling_District_Selangor.html'\n",
    "    \n",
    "    try:\n",
    "        headers = setup_request()\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        time.sleep(5)\n",
    "\n",
    "        restaurant_name = get_restaurant_name(soup)\n",
    "        \n",
    "        # Scrape all reviews across pages\n",
    "        all_reviews = scrape_all_reviews(base_url)\n",
    "\n",
    "        save_to_csv(restaurant_name, all_reviews, 'woodfire_kl_reviews_2a.csv')\n",
    "        print(f\"✅ Reviews for '{restaurant_name}' saved successfully!\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error during requests to {base_url} : {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffadf9d8-be2a-4cd6-b9b0-b5cbf5d7103c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3255f-abb0-4010-9382-57de8a170ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "def setup_request():\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "\n",
    "def get_restaurant_name(soup):\n",
    "    try:\n",
    "        return soup.find('h1').text.strip()\n",
    "    except:\n",
    "        return 'N/A'\n",
    "\n",
    "\n",
    "def scrape_reviews(soup):\n",
    "    reviews = []\n",
    "    review_cards = soup.find_all('div', attrs={'data-automation': 'reviewCard'})\n",
    "\n",
    "    for review in review_cards:\n",
    "        review_data = {\n",
    "            'rating': '',\n",
    "            'title': '',\n",
    "            'text': '',\n",
    "            'date': ''\n",
    "        }\n",
    "\n",
    "        rating_element = review.find('svg', class_='UctUV')\n",
    "        if rating_element:\n",
    "            review_data['rating'] = rating_element.find('title').text.strip().replace(' of 5 bubbles', '')\n",
    "\n",
    "        title_element = review.find('div', attrs={'data-test-target': 'review-title'})\n",
    "        if title_element:\n",
    "            review_data['title'] = title_element.text.strip()\n",
    "\n",
    "        text_element = review.find('div', attrs={'data-test-target': 'review-body'})\n",
    "        if text_element:\n",
    "            review_data['text'] = text_element.text.strip()\n",
    "\n",
    "        date_element = review.find('div', class_='neAPm')\n",
    "        if date_element:\n",
    "            child_divs = date_element.find_all('div')\n",
    "            if child_divs:\n",
    "                review_data['date'] = child_divs[0].text.strip().replace('Written ', '')\n",
    "\n",
    "        reviews.append(review_data)\n",
    "        time.sleep(1)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def get_next_page(soup):\n",
    "    try:\n",
    "        # Find the \"Next\" page link by looking for the \"next\" button or pagination link\n",
    "        next_button = soup.find('a', class_='BrOJk')  # Replace with your found class\n",
    "        if next_button and 'href' in next_button.attrs:\n",
    "            return next_button['href']\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_to_csv(restaurant_name, reviews, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['RESTAURANT_NAME', 'RATING', 'REVIEW_TITLE', 'REVIEW_TEXT', 'REVIEW_DATE'])\n",
    "\n",
    "        for review in reviews:\n",
    "            writer.writerow([\n",
    "                restaurant_name,\n",
    "                review['rating'],\n",
    "                review['title'],\n",
    "                review['text'],\n",
    "                review['date']\n",
    "            ])\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.tripadvisor.com.my/Restaurant_Review-g298570-d25153683-Reviews-Burger_And_Lobster_Klcc-Kuala_Lumpur_Wilayah_Persekutuan.html'\n",
    "    headers = setup_request()\n",
    "\n",
    "    all_reviews = []\n",
    "    page_number = 1\n",
    "\n",
    "    try:\n",
    "        while url:\n",
    "            print(f\"Scraping page {page_number}...\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            restaurant_name = get_restaurant_name(soup)\n",
    "            reviews = scrape_reviews(soup)\n",
    "\n",
    "            all_reviews.extend(reviews)  # Append new reviews to the list\n",
    "\n",
    "            # Check if there's a next page\n",
    "            next_page = get_next_page(soup)\n",
    "            if next_page:\n",
    "                # Ensure we get the full URL if it's a relative link\n",
    "                if not next_page.startswith('http'):\n",
    "                    base_url = 'https://www.tripadvisor.com.my'\n",
    "                    next_page = base_url + next_page\n",
    "                url = next_page\n",
    "                page_number += 1\n",
    "                time.sleep(2)  # Respectful delay between requests\n",
    "            else:\n",
    "                print(\"No more pages found.\")\n",
    "                break\n",
    "\n",
    "        # After scraping all pages, save the reviews to a CSV file\n",
    "        save_to_csv(restaurant_name, all_reviews, 'burger_and_lobster_kl_all_reviews_2a.csv')\n",
    "        print(f\"✅ All reviews for '{restaurant_name}' saved successfully!\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error during requests to {url} : {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be165171-8b53-41af-8fc4-3c92b9449432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found in folder: ['.ipynb_checkpoints', '08622551.pdf', '1-s2.0-S0378437119309999-main.pdf', '1. Regular Expressions - Literals.ipynb', '1.6.2-packet-tracer----configure-basic-router-settings---physical-mode.pka', '14.3.5-packet-tracer---basic-router-configuration-review.pka', '15.6.1-packet-tracer---configure-ipv4-and-ipv6-static-and-default-routes.pka', '1982   2021 Employed persons by industry and state.csv', '1_SMART_Financial_Goals_Worksheet - SN01082115.docx', '1_SMART_Financial_Goals_Worksheet - SN01082115.pdf', '1_SMART_Financial_Goals_Worksheet.docx', '2. Regular Expressions - Metacharacters.ipynb', '2.2.13 Packet Tracer - Point-to-Point Single-Area OSPFv2 Configuration.pdf', '2.2.13 Packet Tracer - Point-to-Point Single-Area OSPFv2 Configuration_COMPLETE.pdf', '2.3.11 Packet Tracer - Determine the DR and BDR.pdf', '2.3.11 Packet Tracer - Determine the DR and BDR_COMPLETE.docx', '2.3.11 Packet Tracer - Determine the DR and BDR_COMPLETE.pdf', '2.4.11 Packet Tracer - Modify Single-Area OSPFv2.docx', '2.4.11 Packet Tracer - Modify Single-Area OSPFv2_COMPLETE.pdf', '2.5.3 Packet Tracer - Propagate a Default Route in OSPFv2.docx', '2.5.3 Packet Tracer - Propagate a Default Route in OSPFv2.pdf', '2.5.3 Packet Tracer - Propagate a Default Route in OSPFv2_COMPLETE.pdf', '2105.01274v1.pdf', '2324S1 CSEB3313 Course Outline.docx', '3.3.12-packet-tracer---vlan-configuration.pdf', '3.4.5-packet-tracer---configure-trunks.pdf', '4.3.8-packet-tracer---configure-layer-3-switching-and-inter-vlan-routing.pka', '4.Appendix 4 FYP Logbook - Sem 1 2024 2025.docx', '5. Appendix 6 Report Template FYP1 (1).docx', '5. Appendix 6 Report Template FYP1.docx', '560302.pdf', '6120839240666431282.jpg', '6278570553107398543.jpg', '6336734808411062170.jpg', '6336734808411062171.jpg', '6336734808411062172.jpg', '6336734808411062173.jpg', '6336734808411062174.jpg', '6336734808411062175.jpg', '6336970383072280899.jpg', '6336970383072280900.jpg', '7.2.10-packet-tracer---configure-dhcpv4.pka', '7.4.1-packet-tracer---implement-dhcpv4.pka', '8. SUPPORT VECTOR MACHINE.ipynb', '9.3.3-packet-tracer---hsrp-configuration-guide (completed.pka', '9.3.3-packet-tracer---hsrp-configuration-guide.pka', 'A118755.txt', 'A4095.txt', 'A480.txt', 'Academic problem M.docx', 'Admin login', 'Admin login (2).zip', 'Admin login.zip', 'Admin Main Page', 'Admin Main Page.zip', 'admin.jpg', 'AdminPage.html', 'Adobe Scan Feb 19, 2025 (1)_watermark.pdf', 'Adobe Scan Feb 19, 2025 (1)_with_watermark.pdf', 'Adobe Scan Feb 19, 2025 (2)_watermark.pdf', 'Adobe Scan Feb 19, 2025 (2)_with_atermark.pdf', 'Adobe Scan Feb 19, 2025 (3) (2)_watermark.pdf', 'Adobe Scan Feb 19, 2025 (3) (2)_with_watermark.pdf', 'Adobe Scan Feb 19, 2025_watermark.pdf', 'Adobe Scan Feb 19, 2025_with_watermark (1).pdf', 'Adobe Scan Feb 19, 2025_with_watermark.pdf', 'Adobe Scan Mar 06, 2024 (1).pdf', 'Adobe Scan Mar 06, 2024 (2).pdf', 'Adobe Scan Mar 06, 2024.pdf', 'aggregator project', 'AI Past Year sem 1 2223.pdf', 'AI Past Year sem 2 2223.pdf', 'Anaconda3-2024.06-1-Windows-x86_64.exe', 'Analysis.xaml.cs', 'android-studio-2024.3.1.13-windows.exe', 'Apache-NetBeans-20r1-bin-windows-x64.exe', 'appflask.py', 'appicon.svg', 'appiconfg.svg', 'apple logo.jpeg', 'App_Navigation_Flowchart.png', 'ARASETV42_N1_P1_13.pdf', 'ArrayOfObjects.java', 'Artificial Neural Network-new.pptx', 'ASSIGNMENT 1 - ETHICS.docx', 'Assignment 1 internet protocols.docx', 'Assignment 1-Internet Protocol.docx', 'Assignment 1.pdf', 'Assignment 2 (1).pdf', 'Assignment 2 (2).pdf', 'ASSIGNMENT 2 - Cover Page.docx', 'Assignment 2.pdf', 'ASSIGNMENT 2_Sem 2 Year 2023_2024.pdf', 'assignment_code.ipynb', 'assignment_draft_ml.zip', 'Assignment_slides.pptx', 'Assignment_slides.zip', 'Availability exercise.docx', 'bank statement bank islam mar 24.pdf', 'barbel.png', 'BeffyBull63k', 'Beyond High School.pptx', 'blood_donations_state.csv', 'Book 1(Gantt Chart).csv', 'Book1.twbx', 'Book2.twbx', 'Book3.twbx', 'Book4.twbx', 'Bookshop.csvx', 'Bookshop.xlsx', 'bookstore.csvx', 'bookstore.xlsx', 'Boston.csv', 'Breast_Cancer_Data.csv', 'burgergooglepreprocess.ipynb', 'burger_and_lobster_kl_all_reviews.csv', 'burger_and_lobster_kl_all_reviews.xls', 'burger_bakar_abang_burn_reviews.csv', 'burger_bakar_abang_burn_reviews.xls', 'burger_ji_legend_all_reviews.csv', 'burger_ji_legend_all_reviews.xls', 'burger_on_16_reviews.csv', 'burger_on_16_reviews.xls', 'Business_Proposal.pdf', 'business_proposal_all.txt', 'business_proposal_page_2.txt', 'calendaricon.png', 'CamScanner 02-23-2025 22.04_watermark.pdf', 'CamScanner 03-08-2024 15.26.pdf', 'CamScanner 03-08-2024 15.26_2.pdf', 'CamScanner 03-08-2024 15.26_3.pdf', 'CamScanner 03-08-2024 15.37.pdf', 'CamScanner 03-08-2024 16.29.pdf', 'cedarlssetup.exe', 'Celcom Payment via Click and Pay __ Payment Result.pdf', 'CGEB4112, CGNB422 - Energy and computing.pdf', 'CGEB4112,CGNB422.pdf', 'CGEB4112_CGNB422_Class Activity_291223.pdf', 'Chapter 1.pptx', 'Chapter 2 - Application Layer.ppt', 'Chapter 2 - Requirement analysis.pdf', 'Chapter 2 - Requirement analysis.pptx', 'Chapter 2a - Web Programming.docx', 'Chapter 2a - Web Programming.pdf', 'Chapter 3 - Logical network design.pptx', 'Chapter 3 Working with Data Sources (Part1).pptx', 'Chapter 3 Working with Data Sources (Part2) New.pptx', 'Chapter 4 - Network Layer (1).ppt', 'Chapter 4 - Network Layer.ppt', 'Chapter 4 Chart Visualization and Plotting (Part 1).pptx', 'Chapter 4 Chart Visualization and Plotting (Part 2).pptx', 'Chapter 4 Stacks.pptx', 'Chapter 4.twbx', 'Chapter 5 Queues.pptx', 'Chapter 6 Tree (Part 1).pptx', 'Chapter 6 Tree (Part 2).pptx', 'Chapter 7 Graph.pptx', 'Chapter 8 Sorting.pptx', 'Chapter 9.pptx', 'Chapter2_ Interaction Design Basic.pptx', 'Chapter3_ Cognitive Models.pptx', 'Chapter4_ HCI In Software Process.pptx', 'Chapter5_ Design Rules.pptx', 'Chapter6_ Evaluation techniques (1).pptx', 'Chapter6_ Evaluation techniques (2).pptx', 'Chapter6_ Evaluation techniques.pptx', 'Chapter7_ Universal Design (1).pptx', 'Chapter7_ Universal Design.pptx', 'Chapter8_ User Support.pptx', 'chromedriver-win64', 'chromedriver-win64.zip', 'ChromeSetup.exe', 'Circuits.pdf', 'CISB3323 HCI Carry Marks.xlsx', 'CiscoPacketTracer_821_Windows_64bit.exe', 'citation-344989540.bib', 'citation.bib', 'Claritas - IO - Wan Emir Hussein bin Wan Rahim - 20240619.pdf', 'Class w4 dvp.twbx', 'client.cpp', 'college_gpa_(gpa-calculator.com).pdf', 'Company_data.csv', 'Copy of Engineering Project Proposal Presentation (1).pptx', 'Copy of Engineering Project Proposal Presentation.pptx', 'COURSE OUTLINE AKPK1011-KPKB111  rusli  (1).docx', 'COURSE OUTLINE AKPK1011-KPKB111  rusli .docx', 'CS01083242-EXERCISE-8.ipynb', 'cs7638-indiana-drones.pdf', 'CSNB4423 Section 1.xlsx', 'CSNB544CSNB5123 Final Project Paper -sem 2 2023-2024-revise 1 (1).docx', 'CSNB544CSNB5123 Final Project Paper -sem 2 2023-2024-revise 1.docx', 'CSNB544CSNB5123 Final Project Paper -sem 2 2023-2024-SN01082115.pdf', 'CSNB544CSNB5123 Milestone 1 (1).docx', 'CSNB544CSNB5123 Milestone 1.docx', 'CSNB544CSNB5123 Milestone 2 (1).docx', 'CSNB544CSNB5123 Milestone 2 - SN01082115.pdf', 'CSNB544CSNB5123 Milestone 2.docx', 'CSNB544CSNB5123-Lab0 - IDE Setup (Repaired).docx', 'CSNB544CSNB5123-Lab0 - IDE Setup.docx', 'CSNB544CSNB5123-Lab08 - Final Task - SN01082115.pdf', 'CSNB544CSNB5123-Lab08.docx', 'CSNB544CSNB5123-Lab1 - Hello World.docx', 'CSNB544CSNB5123-Lab2 - Design Your Application (1).docx', 'CSNB544CSNB5123-Lab2 - Design Your Application (2).docx', 'CSNB544CSNB5123-Lab2 - Design Your Application.docx', 'CSNB544CSNB5123-Lab3 - Form Layout edit.docx', 'CSNB544CSNB5123-Lab3 - Form Layout.docx', 'CSNB544CSNB5123-Lab4 - Form Activity (1).docx', 'CSNB544CSNB5123-Lab4 - Form Activity - SN01082115.docx', 'CSNB544CSNB5123-Lab4 - Form Activity - SN01082115.pdf', 'CSNB544CSNB5123-Lab4 - Form Activity.docx', 'CSNB544CSNB5123-Lab5 - Navigation - SN01082115.pdf', 'CSNB544CSNB5123-Lab5 - Navigation.docx', 'CSNB544CSNB5123-Lab6 - Storage (File) - SN01082115.pdf', 'CSNB544CSNB5123-Lab6 - Storage (File).docx', 'CSNB544CSNB5123-Lab7 - Storage (Firebase) (Repaired).docx', 'CSNB544CSNB5123-Lab7 - Storage (Firebase) - SN01082115.pdf', 'CSNB544CSNB5123-Lab7 - Storage (Firebase).docx', 'CSNB594CSNB4423-Lab_1_Parallel_Computing_Design.docx', 'CSNB594CSNB4423_Lab_2_Section_2_Wan_Emir_Hussein.pdf', 'CSNB594_4423-Assignment 1 ANSWER.docx', 'Customers.csv', 'cv-joblum-137813-XxxkTCYAOmhB.docx', 'data struct.zip', 'data.csv', 'dataAggregator.py', 'DATT Analysis.docx', 'DD and MS (1).concepts', 'DD and MS (1).zip', 'DD and MS.concepts', 'dd.rar', 'Deep learning.pptx', 'Demo on Decision Tree.ipynb', 'Demo-Logistic regression.ipynb', 'Demo-preprocessing example.ipynb', 'demoAdminPage.html', 'Depression Student Dataset.csv', 'desktop.ini', 'diabetes.csv', 'diagram-export-08-12-2023-8_07_25-pm.png', 'diagrams_system.drawio', 'DIPLOMA TRANSCRIPT.pdf', 'DOC-20241210-WA0010. (1)-1-6-6 (1).pdf', 'document.pdf', 'document_vectors.csv', 'dokumen_tazu_2025.pdf', 'download (1).jpg', 'download.jpg', 'downloads.jpg', 'Draft slides presentation m2.pptx', 'DSpace user self registration.pptx', 'edgedriver_win32', 'edgedriver_win32.zip', 'Emir IC (1).pdf', 'Emir Resume (Jun 2023).docx', 'Emir Resume (Jun 2023).pdf', 'Emir Resume 2024.pdf', 'Emir_Resume_2024 - emir wan.docx', 'Emir_Resume_2024.pdf', 'Emir_Resume_Alignerr_v2.docx', 'Emir_Resume_Alignerr_v2.pdf', 'emu8086.exe', 'Energy and Computing (Mini Project).txt', 'Energy and computing Sem 1 2223.pdf', 'Energy and computing Sem 2 2223.pdf', 'Error when installing WAMPSERVER.pdf', 'estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition.zip', 'estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.csv', 'Example_SMART_Financial_Goals_Worksheet.pdf', 'EXERCISE 1-ver2 (1).ipynb', 'EXERCISE 1-ver2-Emir.ipynb', 'EXERCISE 1-ver2.ipynb', 'EXERCISE 2 -KNN.ipynb', 'EXERCISE 8.ipynb', 'Exercise Lab Module 4-Linear Regression (1).pdf', 'Exercise Lab Module 4-Linear Regression.pdf', 'FHRP.mp4', 'FIELD WORK PROJECT PERSONAL REFLECTION REPORT - sn01082115.docx', 'Film Festival (1).zip', 'Film Festival.zip', 'Film_Festival', 'film_festival.sql', 'Film_Festival.zip', 'fitness.xlsx', 'Flipped Class Lesson Plan 4-CISB3323 HCI- Chapter 9.pdf', 'Flipped Classroom Lesson Plan 3.pdf', 'Flow1(1).tflx', 'Flow1(2).tflx', 'Flow1.tflx', 'flutter_windows_3.29.1-stable', 'flutter_windows_3.29.1-stable.zip', 'Forecasting-daily-foot-traffic-in-recreational-trails-using-machine-learning.pdf', 'fuelshack_kl_all_reviews.csv', 'fuelshack_kl_all_reviews.xls', 'fuelshack_kl_all_reviews.xlsx', 'FY1 DRAFT - Emir.docx', 'FYP1_EMIR_LATEST_FORMAT-full-Moderated (update).docx', 'FYP1_EMIR_LATEST_FORMAT-full-Moderated.pdf', 'FYP1_slides_presentation_Emir.pptx', 'gastro_sentral_all_reviews.csv', 'gastro_sentral_all_reviews.xls', 'GBjSLxXakAAddno.jpeg', 'GCM89LPaMAAGV77.jpeg', 'GC_MdoRbUAAimEA.jpeg', 'Git-2.45.2-64-bit.exe', 'google logo.jpeg', 'google_maps_data.csv', 'Group_Assignment_MLDA.zip', 'Guide for USB Debugging and HyperV.txt', 'HCI sem 1 2223.pdf', 'HCI Sem 2 2223.pdf', 'house_data.csv', 'hyperparameters-tuning-example.ipynb', 'IC-Project-Timeline-Gantt-Chart-Template-for-Excel-11412(Project Timeline Gantt - EX).csv', 'icdm_paper.pdf', 'IEEE Xplore Citation BibTeX Download 2024.11.23.19.25.46.bib', 'IEEE Xplore Citation BibTeX Download 2024.11.23.19.35.25.bib', 'IEEE Xplore Citation BibTeX Download 2024.11.23.19.38.21.bib', 'IEEE Xplore Citation BibTeX Download 2024.11.23.19.5.45.bib', 'Iftar Perdana (1).pdf', 'Iftar Perdana (2).pdf', 'Iftar Perdana.pdf', 'ilovepdf_extracted-pages.zip', 'ilovepdf_merged (1).pdf', 'ilovepdf_merged (2) (1).pdf', 'ilovepdf_merged (2).pdf', 'ilovepdf_merged.pdf', 'ilovepdf_watermark', 'Improving_Urban_Crowd_Flow_Prediction_on_Flexible_Region_Partition.pdf', 'INDUSTRIAL TRAINING COMPLETION ACKNOWLEDGEMENT-emir.docx', 'INDUSTRIAL TRAINING COMPLETION ACKNOWLEDGEMENT-emir.pdf', 'Introduction_to_Numpy_and_Pandas (6).ipynb', 'Iris_Data.csv', 'IT_Visit_Presentation_Wan_Emir_Hussein_SN01082115 (1).pptx', 'jdk-21_linux-aarch64_bin.tar.gz', 'JOYAHS_BUDGET_EMIR_SN01082115.docx', 'Kfold-cross-validation-example.ipynb', 'KNN-regression example.ipynb', 'Lab 08 - Final Task-20240606', 'Lab 08 - Final Task-20240606.zip', 'Lab 1 Text Analytics (1).ipynb', 'Lab 1 Text Analytics (2).ipynb', 'Lab 1 Text Analytics.ipynb', 'Lab 1 Working with Text Data.pdf', 'LAB 1.pdf', 'LAB 12 Exercise on Clustering .ipynb', 'lab 1435.pka', 'lab 1561.pka', 'Lab 2 TA.ipynb', 'Lab 2(SN01081012) (1).docx', 'Lab 2(SN01081012).pdf', 'Lab 2(SN01082115).docx', 'Lab 2-KNN -DEMO.ipynb', 'Lab 3 Text Analytics.ipynb', 'Lab 3.1 Text Analytics.ipynb', 'Lab 361 SN01082115.pka', 'Lab 4 - Basic Text Pre Processing.pdf', 'Lab 4 - Review.ipynb', 'Lab 4 - UNITENReview.ipynb', 'Lab 4.twbx', 'Lab 427 SN01082115.pka', 'Lab 438.pka', 'Lab 5 - Advanced Text Pre Processing.pdf', 'Lab 5 Exercise 1 - Web Programmin.txt', 'Lab 5-Model Validation -Linear regresion and cross validation (1).pdf', 'Lab 5-Model Validation -Linear regresion and cross validation.pdf', 'Lab 5.pdf', 'Lab 6 Exercise Q1 Output.png', 'Lab 6 Exercise Q1.txt', 'Lab 6 Exercise Q2 Output.png', 'Lab 6 Exercise Q2.txt', 'lab 7 ddmp.png', 'lab 7210 sn01082115.pka', 'lab 741 sn01082115.pka', 'Lab 8 Exercise Q1 DDMP output.png', 'Lab 8 Exercise Q1 DDMP.txt', 'Lab 8 Exercise Q2 DDMP Output.png', 'Lab 8 Exercise Q2 DDMP.txt', 'Lab 8 Exercise Q3 DDMP Output.png', 'Lab 8 Exercise Q3 DDMP.txt', 'lab 9 ddmp.png', 'Lab Assignment 1 (2).ipynb', 'Lab Assignment 1 - Web Scraping.pdf', 'Lab Assignment 1(1).ipynb', 'Lab Assignment 1.ipynb', 'Lab Module 4-DEMO-LINEAR REGRESSION WITH COMPANY DATA TO PREDICT SALES.ipynb', 'LAB TEST.pka', 'Lab00 Wan Emir Hussein SN01082115 Mobile App Dev.pdf', 'Lab1_Q1_CS01082169.cpp', 'Lab1_Q2_CS01082169.cpp', 'Lab1_Q3_CS01082169.cpp', 'Lab2_Q1.cpp', 'Lab2_Q2.cpp', 'Lab2_Q3.cpp', 'Lab3_Q1.cpp', 'Lab3_Q2.cpp', 'Lab3_Q3.cpp', 'Lab4_AI (1).ipynb', 'Lab5.ipynb', 'Lab5Exercise1-WebProgramming.html', 'LabModule4-CombinationalCircuit(PartI) (1) (2).pdf', 'Lab_5_Neural_Networks_SN01082115_Emir.ipynb', 'lazada_products.csv', 'lazada_reviews(1).csv', 'lazada_reviews.csv', 'Lecture - Regular Expressions.ipynb', 'LOG Book - Emir.docx (1).pdf', 'LOG Book - Emir.docx.pdf', 'LOGIN', 'login system', 'login system.zip', 'LOGIN.zip', 'LogisticRegression Example (1).ipynb', 'LogisticRegression Example.ipynb', 'lotusposition.png', 'M2U_20240522_0122.pdf', 'Machine learning and data analytics - CSNB5213 -.pdf', 'main (5).cpp', 'main.cpp', 'main2.cpp', 'Maps_tutorial.twbx', 'Map_tutorial_superstore.twbx', 'MARA - MyEduloan Portal - Surat Tawaran.pdf', 'Meeting in _General_-20240107_182414-Meeting Recording.mp4', 'Meeting in _video hci fc4_-20240106_134013-Meeting Recording.mp4', 'mendeley-reference-manager-2.126.0-x64.exe', 'mince_meat_reviews.csv', 'mince_meat_reviews.xls', 'Mini Project (Energy and Computing).pdf', 'mini project video part 1 -Emir.mp4', 'mini project video part 2 - Emir.mp4', 'mini project video part 3 - Emir.mp4', 'mini project video part 4 - Emir.mp4', 'mini project video part 5 - Emir.mp4', 'MLDA-K-Nearest Neighbors (KNN) Regression with Scikit-Learn.pptx', 'MLDA-module 1-student-copy (1).pptx', 'MLDA-module 1-student-copy.pptx', 'MLDA-module 2-updatedoct-2024.pptx', 'MLDA_Module 6_Train_Test_Validate-updated-nov2024.pptx', 'ML_S2_23_24.pdf', 'Modelling_Crowd_Scenes_for_Event_Detection.pdf', 'Module 3-ML details-updated-oct2024-updated.pptx', 'Module 3-ML details-updated-oct2024.pptx', 'Module 4 -Linear Regression-28-10-24.pptx', 'Module 4 -Linear Regression-LATEST-update.pptx', 'Module 5.pptx', 'Module 7- Introduction to Data Preprocessing.pptx', 'Mount Rainier Data.xlsx', 'movie.jpg', 'msg44487176-68690.jpg', 'msg44487176-70569.jpg', 'msg44487176-70570.jpg', 'msg44487176-70571.jpg', 'msg44487176-76384.jpg', 'msg895915636-74772.jpg', 'msg895915636-74774.jpg', 'mulahFirstPage.html', 'mulahpageone.js', 'mulahpagethree.js', 'mulahpagetwo.js', 'mulahSecondPage.html', 'mulahThirdPage.html', 'myburgerlab_all_reviews.csv', 'myburgerlab_all_reviews.xls', 'Naïve Bayes Classifier Algorithm-NEW (1).pptx', 'Naïve Bayes Classifier Algorithm-NEW.pptx', 'NetTictactoe.exe', 'New channel meeting-20240106_134548-Meeting Recording.mp4', 'news.xml', 'Non-Profit Donations South.csv', 'NoteGPT-Flowchart-1734618522572.png', 'ObesityDataSet (1).csv', 'ObesityDataSet.csv', 'Obesity_Dataset.xlsx', 'OOP sem 1 2223.pdf', 'OOP sem 2 2223.pdf', 'OOP_Chapter07 Inheritance.ppt', 'OOP_Chapter09 File.ppt', 'Orange_Telecom_Churn_Data.csv', 'Orders.csv', 'Page_1.jpg', 'Parallel computing Sem 1 2223.pdf', 'Parallel Computing Sem 2 2223.pdf', 'Parallel Computing Unit 2 - Parallel Computing Architecture (rina).pptx', 'Parallel Computing Unit 3 - Principles of Parallel Computing Design.pptx', 'Parallel Computing Unit 4 - Pthreads.pptx', 'Parallel Computing Unit 5 - OpenMP.pptx', 'Past_Year_Practical_OOP.pdf', 'Pedestrian-Traffic-Indicator.pptx', 'Pedestrian_analysis_for_crowd_monitoring_the_Milan_case_study_Italy.pdf', 'Pedestrian_Navigation_ERD.png', 'Peer_Evaluation_SN01082115.pdf', 'Pekeliling-Am-Negeri-Johor-Bil-3-2023.pdf', 'PERMOHONAN_BANTUAN_SEMESTER_TAZU_2024_WAN_EMIR_HUSSEIN_BIN_WAN_RAHIM_SN01082115.pdf', 'Permohonan_Bantuan_Zakat_000617141197 (1).pdf', 'Permohonan_Bantuan_Zakat_000617141197.pdf', 'petronas logo.jpeg', 'photo1710917064.jpeg', 'photo1714748914.jpeg', 'Pieces_Suite.appinstaller', 'pima-indians-diabetes.csv', 'Polynomial Regression-update.pptx', 'Polynomial-LR-Cubiq.ipynb', 'Processed_Reviews (1).csv', 'Processed_Reviews.csv', 'Processed_UNITENReviews.csv', 'project rubric.xlsx', 'Project1Briefing-SNCSDepartment_SEM1_2425-latest (1).pptx', 'Project1Briefing-SNCSDepartment_SEM1_2425-latest.pptx', 'Proposal AKPK.pptx', 'pthreads examples.docx', 'python-3.12.4-amd64.exe', 'qbittorrent_5.0.4_x64_setup.exe', 'quiz 2 - lol.twbx', 'receipt_FYP1-EMIR-LATEST_FORMAT - Copy.docx.pdf', 'receipt_FYP1-EMIR-LATEST_FORMAT - Copy_latest.docx.pdf', 'Report Assignment MLDA.docx', 'Report Assignment MLDA.pdf', 'Report Intern - IS01081521.pdf', 'Report internship.docx', 'Report internship.pdf', 'Report Template 2021.docx', 'Requirement analysis exercise.docx', 'Research On SQL Injection Vulnerabilitie.pdf', 'resetPassword_admin.html', 'restaurants_urls.txt', 'Revenue.csv', 'Review.csv', 'Review.xlsx', 'reviews.csv', 'reviews.xlsx', 'Revision Exercise_Exam Questions Compilation.pdf', 'rubrics for class project_sem 2 2122 (1).xlsx', 'rubrics for class project_sem 2 2122 (2).xlsx', 'rubrics for class project_sem 2 2122.xlsx', 'Sample - Superstore.tds', 'Sample - Superstore.xls', 'Sample Report-BSH.pdf', 'Sample Report.pdf', 'sample.txt', 'Screenshot 2024-05-07 001934.png', 'Screenshot 2024-05-07 152550.png', 'Screenshot 2024-05-07 152633.png', 'Screenshot 2024-05-07 152756.png', 'Screenshot 2024-05-07 164525.png', 'Screenshot 2024-10-28 182411.png', 'Screenshot 2025-01-13 011648.png', 'Screenshot 2025-02-22 204730 (1).pdf', 'Screenshot 2025-02-22 204730 (1).png', 'Screenshot 2025-02-22 204730.png', 'Section 01A Lab 2 Data Preprocessing and Cleaning.docx', 'Section 01A Lab 3 Join, Relationships and Union.docx', 'Section 01B Lab 1 Introduction To Tableau.pdf', 'Section 01B Lab 1 Introduction To Tableau_COMPLETE.docx', 'Section 01B Lab 1 Introduction To Tableau_COMPLETE.pdf', 'Section 01B Lab 1 Introduction To Tableau_COMPLETE_SN01082115.pdf', 'Setting-Your-Financial-Goals.pptx', 'Signed_Project_Proposal_Form_2024_2025_SN01082115_Wan_Emir_Hussein (1).pdf', 'Signed_Project_Proposal_Form_2024_2025_SN01082115_Wan_Emir_Hussein.docx', 'Signed_Project_Proposal_Form_2024_2025_SN01082115_Wan_Emir_Hussein.pdf', 'slides akpk chapter 2.pptx', 'Slides AKPK Topik 2 Group 2.pptx', 'SN01082115-W11-01.jpg', 'SN01082115-W11-02.jpg', 'SN01082115_SN01083144.pka', 'SN01082646_LAB 12 .ipynb', 'social_data.json', 'Socket C++ - Visual Studio 2019', 'Socket C++ - Visual Studio 2019.zip', 'spades_burger_reviews.csv', 'spades_burger_reviews.xls', 'splash0.svg', 'splash_nutrilog2.svg', 'stored_city.txt', 'stored_pdf_text.txt', 'stored_reviews.txt', 'stored_text.txt', 'stored_titles.txt', 'storyboard.drawio (1).pdf', 'student-por.csv', 'student.png', 'Style2Adm.txt', 'styleAdm.css', 'stylesheet.css', 'stylo.css', 'Suntory e-Invoice Test Script_with_Issue_Log.xlsx', 'Support Vector Machine (1).pptx', 'Support Vector Machine.pptx', 'svgfiles_2023-12-21-04-20-03-230179-13731304456475995275.svg', 'svgfiles_2023-12-21-04-23-45-428226-11751189282174562034.svg', 'TableauDesktop-64bit-2024-3-3 (1).exe', 'TableauDesktop-64bit-2024-3-3.exe', 'TableauPrep-2024-3-3.exe', 'TableauPublicDesktop-64bit-2025-1-0.exe', 'Table_Display.html', 'Table_Display1.html', 'Table_Input.csv', 'TCP TWO WAY.docx', 'tcptwoway.png', 'Telegram Desktop', \"Template Sponsor Duit BAKSIS 24' - EXTERNAL SPONSOR - BANK MUAMALAT.docx\", \"Template Sponsor Duit BAKSIS 24' - EXTERNAL SPONSOR - CIMB BANK.docx\", \"Template Sponsor Duit BAKSIS 24' - EXTERNAL SPONSOR - YAYASAN AL-MUSTA'AN PETALING JAYA.docx\", \"Template Sponsor Duit BAKSIS 24'.docx\", 'tf_scores.csv', 'Tictactoe', 'Tictactoe (1)', 'Tictactoe (1).zip', 'Tictactoe.zip', 'Topic 1 - Introduction to Energy Computing_ver3.pptx', 'Topic 1 Intro to Text Analytics.pptx', 'Topic 2 Text Extraction and Web Scraping.pptx', 'Topic 3 Text Pre-Processing Techniques (1).pptx', 'Topic 3 Text Pre-Processing Techniques.pptx', 'Topic 3.1 - Roles_of_Computing_in_Electricity_March 2022.pptx', 'Topic 3.3 - Roles of computing for Nuclear.pptx', 'Tutorial on ANN.pptx', 'UDP TWO WAY.docx', 'udptwoway.png', 'UFCE Test Case (1).xlsx', 'UFCE Test Case.xlsx', 'Unconfirmed 343324.crdownload', 'Unconfirmed 415218.crdownload', 'UNITENReview.csv', 'Untitled (1).ipynb', 'Untitled (1).png', 'Untitled Diagram.drawio', 'Untitled Folder', 'Untitled presentation.pptx', 'Untitled presentation.pptx (1).mp4', 'Untitled presentation.pptx.mp4', 'Untitled.ipynb', 'Untitled.png', 'Untitled1.ipynb', 'Untitled2.ipynb', 'Untitled3 (1).ipynb', 'Untitled3.ipynb', 'Untitled4.ipynb', 'Untitled5.ipynb', 'Untitled6.ipynb', 'UNTITLEDUntitled spreadsheet.xlsx', 'Updated_ERD_Diagram.png', 'Use Case Diagram fyp1(1).png', 'Use Case Diagram fyp1.png', 'User Guideline MCT Widget using HTML.pptx', 'Video Presentation (Fauzan).zip', 'video6111547684055682026.mp4', 'video6145408179720162250.mp4', 'Violet_Black_Geometric_Simple_Presentation_20250103_161329_0000.pptx', 'Visual-C-Runtimes-All-in-One-Feb-2024', 'Visual-C-Runtimes-All-in-One-Feb-2024.zip', 'VisualStudioSetup (1).exe', 'VisualStudioSetup (2).exe', 'VisualStudioSetup.exe', 'Vocaroo 1cpzVltkSakg.mp3', 'Vocaroo 1cwMxAuLFlz3.mp3', 'VSCodeUserSetup-x64-1.98.0.exe', 'VSCodeUserSetup-x64-1.98.2.exe', 'wampserver3.3.5_x64.exe', 'Wan Emir  Hussein - 2025-03-18.pdf', 'why_did_i_take_this_course (1).ipynb', 'why_did_i_take_this_course.ipynb', 'windows_10_cmake_Release_Graphviz-12.2.1-win64.zip', 'winrar-x64-624.exe', 'Wireshark-4.4.3-x64.exe', 'woodfire_kl_all_reviews.csv', 'woodfire_kl_all_reviews.xls', 'wps_lid.lid-s8Bk6ZwaQcbq.exe', 'xampp-windows-x64-8.0.30-0-VS16-installer.exe', 'Zoom_cm_fo42pnktZ9vvrZo4_mppHXyDfBMkt1PPClrz6jtku39rV328prd8oS@EAkS+E6IG6t7cjuD_ka7cc3a9ad7bd815b_.exe', '[OF]Beffy Bull 63k.torrent', '~$ Appendix 6 Report Template FYP1 (1).docx', '~$ Appendix 6 Report Template FYP1.docx', '~$Chapter 3 Working with Data Sources (Part2) New.pptx', '~$Chapter 4 Chart Visualization and Plotting (Part 1).pptx', '~$Chapter 4 Chart Visualization and Plotting (Part 2).pptx', '~$ction 01B Lab 1 Introduction To Tableau_COMPLETE.docx', '~$ir Resume (Jun 2023).docx', '~$NB544CSNB5123-Lab1 - Hello World.docx', '~$NB544CSNB5123-Lab3 - Form Layout.docx', '~$NB544CSNB5123-Lab4 - Form Activity.docx', '~$P1_EMIR_LATEST_FORMAT-full-Moderated.pdf', '~WRL0170.tmp']\n",
      "Adding file: burger_and_lobster_kl_all_reviews.csv\n",
      "Adding file: burger_bakar_abang_burn_reviews.csv\n",
      "Adding file: burger_ji_legend_all_reviews.csv\n",
      "Adding file: burger_on_16_reviews.csv\n",
      "Adding file: fuelshack_kl_all_reviews.csv\n",
      "Adding file: gastro_sentral_all_reviews.csv\n",
      "Adding file: mince_meat_reviews.csv\n",
      "Adding file: myburgerlab_all_reviews.csv\n",
      "Adding file: spades_burger_reviews.csv\n",
      "Adding file: woodfire_kl_all_reviews.csv\n",
      "✅ Selected CSV files combined successfully into 'tripadvisor_reviews_burgershops_kl.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def list_and_combine_csv_files(input_folder, output_file, include_files):\n",
    "    all_data = []\n",
    "    folder_files = os.listdir(input_folder)\n",
    "\n",
    "    print(f\"Files found in folder: {folder_files}\")  # Debugging step\n",
    "\n",
    "    # Loop through each file in the folder\n",
    "    for file_name in folder_files:\n",
    "        # Check if it's a CSV file and in the list of files to combine\n",
    "        if file_name.endswith('.csv') and file_name in include_files:  \n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            print(f\"Adding file: {file_name}\")  # Optional: print which file is being added\n",
    "            # Read the CSV file into a DataFrame and append it to the list\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_data.append(df)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No matching CSV files found to combine.\")\n",
    "        return\n",
    "\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Selected CSV files combined successfully into '{output_file}'.\")\n",
    "\n",
    "# Usage:\n",
    "input_folder = r'C:\\Users\\tiemi\\Downloads'  # Corrected path with raw string\n",
    "output_file = 'tripadvisor_reviews_burgershops_kl.csv'    # Renamed output file\n",
    "include_files = ['myburgerlab_all_reviews.csv', 'burger_and_lobster_kl_all_reviews.csv', 'burger_bakar_abang_burn_reviews.csv', 'burger_ji_legend_all_reviews.csv', 'burger_on_16_reviews.csv', 'fuelshack_kl_all_reviews.csv', 'gastro_sentral_all_reviews.csv', 'mince_meat_reviews.csv', 'myburgerlab_all_reviews.csv', 'spades_burger_reviews.csv', 'woodfire_kl_all_reviews.csv']  # List the filenames you want to combine\n",
    "\n",
    "list_and_combine_csv_files(input_folder, output_file, include_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
